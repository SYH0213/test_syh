{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6be8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0a482",
   "metadata": {},
   "source": [
    "# 🎙️ Pyannote Speaker Diarization 코드 설명\n",
    "\n",
    "이 코드는 Hugging Face의 **pyannote.audio** 라이브러리를 사용하여  \n",
    "오디오 파일(`audio.wav`)에서 **화자 분리(Speaker Diarization)**를 수행하는 예제입니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 주요 흐름\n",
    "\n",
    "1. **환경 변수 불러오기**\n",
    "   - `.env` 파일에 저장된 `HUGGINGFACE_ACCESS_TOKEN`을 불러와서 Hugging Face 모델 접근에 사용합니다.\n",
    "\n",
    "2. **Pipeline 로드**\n",
    "   - `pyannote/speaker-diarization-3.1` 모델을 불러옵니다.\n",
    "   - 최신 버전의 `pyannote.audio`(3.x 계열)에서는 `plda` 같은 파라미터는 필요하지 않습니다.\n",
    "   - GPU(CUDA)가 가능하다면 자동으로 GPU에서 실행됩니다.\n",
    "\n",
    "3. **오디오 파일 경로 설정**\n",
    "   - `os.getcwd()`를 사용해 현재 작업 디렉토리에서 `audio.wav` 파일을 불러옵니다.\n",
    "\n",
    "4. **Diarization 실행**\n",
    "   - `pipeline(audio_file, hook=hook)`을 실행하면 화자 분리 결과(`Annotation` 객체)가 반환됩니다.\n",
    "   - `ProgressHook`을 사용하면 진행 상황이 표시됩니다.\n",
    "\n",
    "5. **결과 출력**\n",
    "   - 결과는 `output.itertracks(yield_label=True)`로 순회할 수 있습니다.\n",
    "   - 여기서 각 변수의 의미는 다음과 같습니다:\n",
    "\n",
    "---\n",
    "\n",
    "## 출력 구조 설명\n",
    "\n",
    "```python\n",
    "for turn, _, speaker in output.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "```\n",
    "\n",
    "-turn\n",
    "\n",
    "   -Segment 객체 (시작/끝 시간 정보를 담음)\n",
    "\n",
    "   -예: Segment(0.2, 1.5)\n",
    "\n",
    "   -접근: turn.start, turn.end\n",
    "\n",
    "_ (track id)\n",
    "\n",
    "   -동일 speaker 안에서 여러 트랙을 구분할 때 쓰이는 ID\n",
    "\n",
    "   -특별히 사용하지 않으므로 _로 무시\n",
    "\n",
    "-speaker\n",
    "\n",
    "   -화자 레이블 (문자열)\n",
    "\n",
    "   -예: \"SPEAKER_00\", \"SPEAKER_01\"\n",
    "\n",
    "# 예시 출력\n",
    "\n",
    "start=0.2s stop=1.5s speaker_SPEAKER_00\n",
    "\n",
    "start=1.8s stop=3.9s speaker_SPEAKER_01\n",
    "\n",
    "start=4.2s stop=5.7s speaker_SPEAKER_00\n",
    "\n",
    "\n",
    "즉, 오디오의 각 구간별로 시작시간 / 종료시간 / 화자ID가 표시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14fedf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 현재 실행 중인 코드 파일 기준 절대경로\n",
    "#오디오 파일 이름\n",
    "AUDIO_PATH = \"data/audio.wav\"\n",
    "BASE_DIR = os.getcwd()\n",
    "# \n",
    "audio_file = os.path.join(BASE_DIR, AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a84ec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844b323312174982870672def111098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\SBA\\github\\test_syh\\sesacproject\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is &lt;= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\SBA\\github\\test_syh\\sesacproject\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: \n",
       "UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input\n",
       "numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
       "  std = sequences.std(dim=-1, correction=1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=8.7s stop=11.9s speaker_SPEAKER_03\n",
      "start=16.1s stop=40.1s speaker_SPEAKER_03\n",
      "start=40.8s stop=48.4s speaker_SPEAKER_00\n",
      "start=48.6s stop=56.5s speaker_SPEAKER_00\n",
      "start=57.0s stop=61.6s speaker_SPEAKER_00\n",
      "start=62.6s stop=71.6s speaker_SPEAKER_01\n",
      "start=72.2s stop=75.7s speaker_SPEAKER_01\n",
      "start=75.9s stop=80.6s speaker_SPEAKER_01\n",
      "start=81.2s stop=85.6s speaker_SPEAKER_01\n",
      "start=86.6s stop=99.3s speaker_SPEAKER_02\n",
      "start=99.9s stop=110.0s speaker_SPEAKER_02\n",
      "start=111.6s stop=118.3s speaker_SPEAKER_03\n",
      "start=118.9s stop=126.8s speaker_SPEAKER_03\n",
      "start=129.2s stop=137.0s speaker_SPEAKER_03\n",
      "start=137.4s stop=141.1s speaker_SPEAKER_03\n",
      "start=142.1s stop=150.6s speaker_SPEAKER_00\n",
      "start=151.3s stop=155.7s speaker_SPEAKER_01\n",
      "start=156.2s stop=160.4s speaker_SPEAKER_01\n",
      "start=161.4s stop=165.9s speaker_SPEAKER_01\n",
      "start=167.3s stop=176.0s speaker_SPEAKER_02\n",
      "start=176.2s stop=184.3s speaker_SPEAKER_02\n",
      "start=186.0s stop=213.0s speaker_SPEAKER_03\n",
      "start=214.4s stop=215.1s speaker_SPEAKER_03\n",
      "start=215.1s stop=216.7s speaker_SPEAKER_03\n",
      "start=217.6s stop=219.5s speaker_SPEAKER_00\n",
      "start=219.6s stop=223.7s speaker_SPEAKER_00\n",
      "start=224.4s stop=227.8s speaker_SPEAKER_01\n",
      "start=228.3s stop=232.0s speaker_SPEAKER_02\n",
      "start=233.1s stop=240.9s speaker_SPEAKER_03\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n",
    "#print(\"HF_TOKEN:\", HF_TOKEN)\n",
    "\n",
    "# Community-1 open-source speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# CUDA가 있으면 GPU, 없으면 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline.to(device)\n",
    "\n",
    "# 실행\n",
    "with ProgressHook() as hook:\n",
    "    output = pipeline(audio_file, hook=hook)\n",
    "\n",
    "# 결과 출력\n",
    "for turn, _, speaker in output.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==============================\n",
    "# 1. 환경 변수 로드\n",
    "# ==============================\n",
    "load_dotenv()\n",
    "PYANNOTE_TOKEN = os.getenv(\"PYANNOTEAI_API_KEY\")\n",
    "\n",
    "if not PYANNOTE_TOKEN:\n",
    "    raise ValueError(\"환경 변수에 PYANNOTEAI_API_KEY가 없습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "# ==============================\n",
    "# 2. Precision-2 Speaker Diarization (서버 실행)\n",
    "# ==============================\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-precision-2\",\n",
    "    token=PYANNOTE_TOKEN\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 3. 오디오 파일 경로\n",
    "# ==============================\n",
    "audio_file = os.path.join(\"data\", \"tiro_Test1_4People.wav\")\n",
    "\n",
    "# ==============================\n",
    "# 4. 실행 (서버에서 처리)\n",
    "# ==============================\n",
    "output = pipeline(audio_file)  # pyannoteAI 서버에서 실행됨\n",
    "\n",
    "# ==============================\n",
    "# 5. 결과 출력\n",
    "# ==============================\n",
    "for turn, speaker in output.speaker_diarization:\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s {speaker}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesacproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
